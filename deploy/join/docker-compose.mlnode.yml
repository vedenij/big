services:
  mlnode-308:
    image: ghcr.io/vedenij/bigmlnode:3.0.13
    hostname: mlnode-308
    ports:
      - "${DELEGATION_PORT:-9090}:8080"
    volumes:
      - ${HF_HOME:-${HOME}/.cache}:/root/.cache
    environment:
      - HF_HOME=/root/.cache
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - DELEGATION_ENABLED=1
      - DELEGATION_MAX_SESSIONS=${DELEGATION_MAX_SESSIONS:-10}
      - DELEGATION_AUTH_TOKEN=${DELEGATION_AUTH_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: uvicorn api.app:app --host=0.0.0.0 --port=8080
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Removed inference service - big node doesn't do inference
  # Delegation API exposed directly on port ${DELEGATION_PORT:-9090}

